{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00704a7",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch (Using only Numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57432f",
   "metadata": {},
   "source": [
    "In this notebook we are going to implement **logistic regression** from scratch, using only Numpy. Logistic regression is equivalent to a **single neuron** of a neural network with a **sigmoid function**.\n",
    "\n",
    "This notebook will set a general idea of what we are going to work on in the upcoming notebooks in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f613f9f",
   "metadata": {},
   "source": [
    "In this Notebook we will implement the following functions:\n",
    "\n",
    "- **`initialize`** → set the initial values of weights `w` and bias `b`  **(For now we will initialize them to zeros)**\n",
    "- **`sigmoid`** → the activation function that maps inputs into probabilities  \n",
    "- **`propagate`** → perform forward propagation (compute predictions & cost) and backward propagation (compute gradients)  \n",
    "- **`optimize`** → update parameters `(w, b)` iteratively using gradient descent  \n",
    "- **`predict`** → use the learned parameters `(w, b)` to predict labels on new data\n",
    "- **`LR_model`** → build a logistic regression model using the functions above\n",
    "- **`plot_learning_curve`** → plot the learning curve of the trained model using costs and matplotlib\n",
    "\n",
    "By the end of this notebook, you’ll see how logistic regression works end-to-end:\n",
    "1. Initialize parameters  \n",
    "2. Learn them through optimization  \n",
    "3. Predict outcomes on unseen examples  \n",
    "\n",
    "This workflow is the **blueprint** for training more complex neural networks in the upcoming notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998994a8",
   "metadata": {},
   "source": [
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "We build each of these seperately, and then put them together in a model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47ee76",
   "metadata": {},
   "source": [
    "# 1 - Initialization of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialize_params(dim):\n",
    "    \"\"\"\n",
    "    Initialize w -> zero vector of shape (dim, 1) and bias b to 0.0\n",
    "\n",
    "    Args:\n",
    "    dim(int): size of the w vector we want (or number of parameters in this case -- number of input features)\n",
    "\n",
    "    Returns:\n",
    "    w: zero vector of shape (dim, 1)\n",
    "    b: float set equal to zero\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros((dim, 1), dtype=float)\n",
    "    b = float(0)\n",
    "\n",
    "    return {'w': w, 'b': b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cd2ea",
   "metadata": {},
   "source": [
    "# 2 - Sigmoid Activation Function\n",
    "We need to implement this function to compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c46650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of z\n",
    "\n",
    "    Args:\n",
    "    z: numpy array\n",
    "\n",
    "    Returns:\n",
    "    s: sigmoid of z\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-z))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a1b6c",
   "metadata": {},
   "source": [
    "# 3 - Forward and Backward propagation\n",
    "After we initialized the **parameters**, we need now to implement the propagate function **to learn them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(W, b, X, Y):\n",
    "    \"\"\"\n",
    "    Forward and Backward propagation, computing the cost function and its gradients\n",
    "\n",
    "    Args:\n",
    "    W: weights, numpy array of size (n_x, 1)\n",
    "    b: bias, scalar\n",
    "    X: input data, numpy array of size (n_x, m)\n",
    "    Y: labels, numpy array of size (1, m)\n",
    "    NOTE: m is the number of examples and n_x = number of features per input = n_px*n_px*3 (Images)\n",
    "\n",
    "    Returns:\n",
    "    cost: computed cost\n",
    "    grads: dictionary containing dW (gradient of the loss with respect to W, same shape as W)\n",
    "                                 db (gradient of the loss with respect to b, same shape as b)\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "\n",
    "    m = X.shape[1]\n",
    "    cost = (-1/m) * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "\n",
    "    dZ = A - Y\n",
    "    dW = (1/m) * np.dot(X, dZ.T)\n",
    "    db = (1/m) * np.sum(dZ)\n",
    "\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    grads = {'dW': dW, 'db': db}\n",
    "\n",
    "    return cost, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e8bab",
   "metadata": {},
   "source": [
    "# 4 - Optimization\n",
    "Now after we finished with forward and backward propagation, we have to implement **optimization** to learn W and b using **gradient descent**, by simply **minimizing the cost**.\n",
    "\n",
    "We simply need to do the following:\n",
    "1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "2) Update the parameters using gradient descent rule for w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2875ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(W, b, X, Y, num_iterations=1000, learning_rate=0.009, print_cost=True):\n",
    "    \"\"\"\n",
    "    Optimize (update) W and b by using gradient descent\n",
    "\n",
    "    Args:\n",
    "    W: weights, numpy array of size (n_x, 1)\n",
    "    b: bias, scalar\n",
    "    X: input data, numpy array of size (n_x, m)\n",
    "    Y: labels, numpy array of size (1, m)\n",
    "    num_iterations: number of times we want to run the optimization\n",
    "    learning_rate: learning rate of W and b (rate of updating)\n",
    "    print_cost: boolean to enable printing the cost every 100 iterations\n",
    "\n",
    "    Returns:\n",
    "    params: dictionary containing W and b\n",
    "    grads: dictionary containing dW and db\n",
    "    costs: list of all costs computed during the optimization, this can be used to plot the learning curve\n",
    "    \n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    W = copy.deepcopy(W)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        cost, grads = propagate(W, b, X, Y)\n",
    "\n",
    "        dW = grads['dW']\n",
    "        db = grads['db']\n",
    "\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                print(f'iteration {i}/{num_iterations}: loss = {cost}')\n",
    "        \n",
    "    params = {'W': W, 'b': b}\n",
    "    grads = {'dW': dW, 'db': db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba18259",
   "metadata": {},
   "source": [
    "# 5 - Making Predictions\n",
    "Finally, after finishing the main functions to train a LR model, We have now to implement the **predict()** function to be able to make predictions.\n",
    "\n",
    "A Logistic regression model makes **prediction based on Y-hat** (A = sigmoid(Z)):\n",
    "- For Y-hat of an entry <=0.5 -> label the entry 0\n",
    "- For Y-hat of an entry > 0.5 -> label the entry 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    \"\"\"\n",
    "    Label entries 0 or 1 based on the value of the activation\n",
    "\n",
    "    Args:\n",
    "    W: weights, numpy array of size (n_x, 1)\n",
    "    b: bias, scalar\n",
    "    X: input data, numpy array of size (n_x, m)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction: numpy array containing predicted labels to all the examples X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] <= 0.5:\n",
    "            Y_prediction[0, i] = 0\n",
    "        else:\n",
    "            Y_prediction[0, i] = 1\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde32be",
   "metadata": {},
   "source": [
    "# 6 - Building the Model\n",
    "Now, after implementing all the core functions of a LR model, we can finally put them together to build a LR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_Model(X_train, X_test, Y_train, Y_test, num_iterations=1000, learning_rate=0.009, print_cost=True):\n",
    "    \"\"\"\n",
    "    Builds LR model with all the previously implemented functions\n",
    "\n",
    "    Args:\n",
    "    X_train:\n",
    "    X_test:\n",
    "    Y_train:\n",
    "    Y_test:\n",
    "    num_iterations:\n",
    "    learning_rate:\n",
    "    print_cost:\n",
    "\n",
    "    Returns:\n",
    "    summary: dictionnary containing -- weights('W'), bias('b'), gradients ('grads'), costs('costs'), train_accuracy('train_accuracy'), \n",
    "                                       test_accuracy('test_accuracy'), Y_prediction_train('Y_prediction_train'), Y_prediction_test('Y_prediction_test'),\n",
    "                                       learning_rate('learning_rate'), and num_iterations('num_iterations').\n",
    "    \"\"\"\n",
    "\n",
    "    m = X_train.shape[0]\n",
    "    W, b = Initialize_params(m)\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    params, grads, costs = optimize(W, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    W = params['W']\n",
    "    b = params['b']\n",
    "    \n",
    "    Y_prediction_train = predict(W, b, X_train)\n",
    "    Y_prediction_test = predict(W, b, X_test)\n",
    "\n",
    "    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100 \n",
    "    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
    "\n",
    "    summary = {\n",
    "        'W': W,\n",
    "        'b': b,\n",
    "        'grads': grads,\n",
    "        'costs': costs,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'Y_prediction_train': Y_prediction_train,\n",
    "        'Y_prediction_test': Y_prediction_test,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_iterations': num_iterations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b830f770",
   "metadata": {},
   "source": [
    "# 7 - Plotting the learning curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(trained_model):\n",
    "    \n",
    "    costs = np.squeeze(trained_model['costs'])\n",
    "    learning_rate = trained_model['learning_rate']\n",
    "    num_iterations = trained_model['num_iterations']\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.title(f'Learing rate = {learning_rate}')\n",
    "    plt.ylabel(costs)\n",
    "    plt.xlabel(num_iterations)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
