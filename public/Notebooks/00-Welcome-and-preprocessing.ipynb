{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc651dc",
   "metadata": {},
   "source": [
    "# Welcome to the DeepLearning NoteBooks series\n",
    "\n",
    "Hi, I’m Elio — a CS student at AUB (Class of 2027), passionate about AI and Deep Learning.  \n",
    "This notebook is the **starting point of my Deep Learning series**, where I refine and share what I’ve learned to:\n",
    "\n",
    "- Build a stronger foundation in ML/DL  \n",
    "- Recap & solidify concepts from the Deep Learning Specialization  \n",
    "- Showcase practical skills through structured notebooks\n",
    "- Provide a clear, step-by-step resource for others to **understand the mathematical concepts and theory** behind        already-implemented methods\n",
    "\n",
    "This is not only a recap of what I have learned from DL specialization by DeepLearning.ai, it provides what is needed for the user to go beyond **libraries** and understand how things really work **under the hood**\n",
    "\n",
    "# What to expect throughout this series of NoteBooks\n",
    "I will be going over foundational DL concepts - explaining the mathematics of each concept, its implementation (most of them with only numpy), and how it works.\n",
    "\n",
    "The series will be consisted of the Following NoteBooks:\n",
    "\n",
    "00. Welcome-and-preprocessing \n",
    "01. Logistic-Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771cc112",
   "metadata": {},
   "source": [
    "# 1 - Data Preprocessing\n",
    "Data preprocessing is the core of **machine learing**. The main step to successfully train an accurate model is to have a good data to train the model on.\n",
    "\n",
    "When it comes to Data, there are **three different types**:\n",
    "1. Images\n",
    "2. Texts\n",
    "3. Numeric/Tabular\n",
    "\n",
    "We are going to go through **the preprocessing of each of these three types of data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c4b36",
   "metadata": {},
   "source": [
    "# 1. Images Preprocessing\n",
    "\n",
    "To preprocess Images, We have to do the following:\n",
    "- Find the dimensions and shapes we are dealing with **(m_train, m_test, num_px, num_px, 3)**\n",
    "- Reshape the data where each example becomes shaped as a vector of size **(num_px*num_px*3)**\n",
    "- Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b357ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images Preprocessing\n",
    "\n",
    "def data_dimensions(train_x, test_x):\n",
    "    m_train = train_x.shape[0]\n",
    "    m_test = test_x.shape[0]\n",
    "    num_px = train_x[0].shape[0]\n",
    "    num_channels = train_x[0].shape[2] # It is by 3 -> RGB\n",
    "\n",
    "    return {'m_train': m_train, 'm_test': m_test, 'num_px': num_px, 'num_channels': num_channels}\n",
    "\n",
    "def reshape_data(train_x, test_x):\n",
    "    train_x_flatten = train_x.reshape(train_x.shape[0], -1).T   # Here -1 = product of the rest dimensions\n",
    "    test_x_flatten = test_x.reshape(test_x.shape[0], -1).T\n",
    "\n",
    "    return (train_x_flatten, test_x_flatten)\n",
    "\n",
    "def standardize_data(train_x_flatten, test_x_flatten):\n",
    "    train_x_std = train_x_flatten / 255.0\n",
    "    test_x_std = test_x_flatten / 255.0\n",
    "\n",
    "    return (train_x_std, test_x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9ba57",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "\n",
    "To preprocess Text, we have to do the following steps:\n",
    "- Tokenize Text **(sentence -> sequence of integers)**\n",
    "- Pad the sequences to the same length\n",
    "\n",
    "The intuition behing each of these 2 steps:\n",
    "- Computers do not understand characters, they process numbers. That's why we tokenize the text sequences and make them sequences of integers. **It maps each unique word to a unique integer ID**\n",
    "- Neural networks expect inputs to have the same shape in a batch, and sentences may differ in length that's why we tend to **pad the sentences to a certain length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388562cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def Tokenize(texts, fitted_tokenizer=None, oov_token = '<OOV>'):\n",
    "    \"\"\"\n",
    "    Convert a list of sentences into integer sequences.\n",
    "    \n",
    "    Args:\n",
    "        texts (list[str]): Sentences to tokenize.\n",
    "        tokenizer (Tokenizer, optional): Existing fitted tokenizer to reuse.\n",
    "        oov_token (str): Token for out-of-vocabulary words (used if tokenizer=None).\n",
    "        \n",
    "    Returns:\n",
    "        tokenizer (Tokenizer): The fitted or reused tokenizer.\n",
    "        sequences (list[list[int]]): List of integer sequences.\n",
    "    \"\"\"\n",
    "    if fitted_tokenizer:\n",
    "        tokenizer = fitted_tokenizer\n",
    "    else:\n",
    "        tokenizer = Tokenizer(num_words=None, oov_token=oov_token)\n",
    "\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    return {'tokenizer': tokenizer, 'sequences': sequences}\n",
    "\n",
    "def Pad(sequences, max_len=None, padding='post'):\n",
    "    \"\"\"\n",
    "    Pad sequences to become of the same length.\n",
    "    short sequences -> padded to same length as the longer squences\n",
    "    \n",
    "    Args:\n",
    "        sequences (list[int]): tokenized sentences.\n",
    "        max_len (int, optional): a desired length for shorter sequences to be padded to and longer ones to be truncated to.\n",
    "                                (if kept None the function will pad the shorter sequences to have same length as the longest one)\n",
    "        oov_token (str): Token for out-of-vocabulary words (used if tokenizer=None).\n",
    "        \n",
    "    Returns:\n",
    "       padded(list[int]): padded sequences (or truncated/padded if max_len != None)\n",
    "    \"\"\"\n",
    "\n",
    "    padded = pad_sequences(sequences, maxlen=None, padding='post')\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fb96d",
   "metadata": {},
   "source": [
    "# 3. Tabular/Numeric Preprocessing\n",
    "\n",
    "To preprocess Numeric/Tabular data (df: dataframe), we have to do the following steps:\n",
    "- Drop rows with **Nan** values\n",
    "- Do one-hot encoding to categorical feauture columns **This is used for data that have one column for each label. (1 for corresponding label/s and 0 for the rest)**\n",
    "(OR)\n",
    "- Do label encoding **This is used for data that have only one column for all labels where each row has a class name under it. (Labels to integers -> each label = unique integer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d09fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular/Numeric Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "def dropNan(df):\n",
    "    \"\"\"\n",
    "    Drop from the dataframe rows that have any NaN values\n",
    "\n",
    "    Args:\n",
    "        df(pd.DataFrame): DataFrame (table)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame without NaN values\n",
    "    \"\"\"\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def one_hot_encoding(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Apply one_hot encoding to categorical feature columns\n",
    "\n",
    "    Args:\n",
    "        df(pd.DataFrame): DataFrame without NaN values\n",
    "        categorical_cols(list[str]): Names of categorical columns to encode (names of classes)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with one-hot encoding applied to it\n",
    "    \"\"\"\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "    return encoded, encoder\n",
    "\n",
    "def label_encoding(labels):\n",
    "    \"\"\"\n",
    "    Encode string labels (train_y / test_y) into integers.\n",
    "    \n",
    "    Args:\n",
    "        labels (pd.Series or list): Target labels.\n",
    "    \n",
    "    Returns:\n",
    "        encoded (ndarray): Integer-encoded labels.\n",
    "        encoder (LabelEncoder): Fitted label encoder (for reuse later).\n",
    "    \"\"\"\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoded = encoder.fit_transform(labels)\n",
    "    \n",
    "    return encoded, encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
